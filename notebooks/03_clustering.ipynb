{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Analysis\n",
    "\n",
    "This notebook applies various clustering algorithms to group similar Suricata rules.\n",
    "\n",
    "We will:\n",
    "1. Load the feature matrix\n",
    "2. Apply K-Means clustering\n",
    "3. Apply DBSCAN clustering\n",
    "4. Apply Hierarchical clustering\n",
    "5. Compare algorithm performance\n",
    "6. Analyze cluster characteristics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import sys\nsys.path.append('..')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom suricata_rule_clustering import parser, clustering, features\nfrom suricata_rule_clustering.cluster_analysis import ClusterDescriptor, format_cluster_description\n\n# Set display options\npd.set_option('display.max_columns', None)\n\n# Set style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load feature matrix and processed DataFrame\n",
    "X = np.load('../data/feature_matrix.npy')\n",
    "df = pd.read_pickle('../data/processed_rules.pkl')\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"DataFrame shape: {df.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Means Clustering\n",
    "\n",
    "First, let's find the optimal number of clusters using the elbow method and silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Find optimal k using elbow method\n",
    "optimal_k_elbow, elbow_scores = clustering.find_optimal_k(\n",
    "    X,\n",
    "    k_range=range(2, 15),\n",
    "    method='elbow'\n",
    ")\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(list(elbow_scores.keys()), list(elbow_scores.values()), 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal k (elbow): {optimal_k_elbow}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Find optimal k using silhouette score\n",
    "optimal_k_silhouette, silhouette_scores = clustering.find_optimal_k(\n",
    "    X,\n",
    "    k_range=range(2, 15),\n",
    "    method='silhouette'\n",
    ")\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(list(silhouette_scores.keys()), list(silhouette_scores.values()), 'go-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score For Different k Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal k (silhouette): {optimal_k_silhouette}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply K-Means with chosen k\n",
    "k = optimal_k_silhouette\n",
    "\n",
    "kmeans_clusterer = clustering.RuleClusterer(\n",
    "    algorithm='kmeans',\n",
    "    n_clusters=k\n",
    ")\n",
    "kmeans_clusterer.fit(X)\n",
    "\n",
    "print(f\"K-Means clustering completed with {k} clusters\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate K-Means clustering\n",
    "kmeans_metrics = clustering.evaluate_clustering(X, kmeans_clusterer.labels_)\n",
    "print(\"K-Means Evaluation Metrics:\")\n",
    "for metric, value in kmeans_metrics.items():\n",
    "    print(f\"  {metric}: {value}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get cluster summary\n",
    "kmeans_summary = kmeans_clusterer.get_cluster_summary(df)\n",
    "print(\"\\nK-Means Cluster Summary:\")\n",
    "kmeans_summary.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DBSCAN Clustering\n",
    "\n",
    "DBSCAN is a density-based algorithm that can find clusters of arbitrary shapes and automatically detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply DBSCAN\n",
    "# You may need to experiment with eps and min_samples parameters\n",
    "dbscan_clusterer = clustering.RuleClusterer(\n",
    "    algorithm='dbscan',\n",
    "    eps=0.5,\n",
    "    min_samples=5\n",
    ")\n",
    "dbscan_clusterer.fit(X)\n",
    "\n",
    "print(\"DBSCAN clustering completed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate DBSCAN clustering\n",
    "dbscan_metrics = clustering.evaluate_clustering(X, dbscan_clusterer.labels_)\n",
    "print(\"DBSCAN Evaluation Metrics:\")\n",
    "for metric, value in dbscan_metrics.items():\n",
    "    print(f\"  {metric}: {value}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get cluster summary\n",
    "dbscan_summary = dbscan_clusterer.get_cluster_summary(df)\n",
    "print(\"\\nDBSCAN Cluster Summary:\")\n",
    "dbscan_summary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Clustering\n",
    "\n",
    "Takes a lot of time, stopped at 40m runtime and disabled for now."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Apply Hierarchical clustering\n",
    "# hierarchical_clusterer = clustering.RuleClusterer(\n",
    "#     algorithm='hierarchical',\n",
    "#     n_clusters=k,  # Use same k as K-Means for comparison\n",
    "#     linkage='ward'\n",
    "# )\n",
    "# hierarchical_clusterer.fit(X)\n",
    "#\n",
    "# print(\"Hierarchical clustering completed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Evaluate Hierarchical clustering\n",
    "# hierarchical_metrics = clustering.evaluate_clustering(X, hierarchical_clusterer.labels_)\n",
    "# print(\"Hierarchical Clustering Evaluation Metrics:\")\n",
    "# for metric, value in hierarchical_metrics.items():\n",
    "#     print(f\"  {metric}: {value}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Get cluster summary\n",
    "# hierarchical_summary = hierarchical_clusterer.get_cluster_summary(df)\n",
    "# print(\"\\nHierarchical Cluster Summary:\")\n",
    "# hierarchical_summary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Plot dendrogram (with sampled data for performance)\n",
    "# fig = clustering.plot_dendrogram(X, max_samples=500)\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Compare Algorithms\n\nThis section compares different clustering algorithms to find the best approach for our Suricata rules.\n\n**Performance Optimizations:**\n- **Dataset Sampling**: Uses a maximum of 10,000 rules (or 20% of dataset) for faster comparison while maintaining statistical validity\n- **PCA Dimensionality Reduction**: Reduces feature dimensions from ~140 to 50, speeding up all distance computations\n- **Mini-Batch K-Means**: Replaces slow Hierarchical clustering (40+ min runtime) with faster Mini-Batch K-Means alternative\n\n**Expected Runtime**: 2-5 minutes (down from 40+ minutes)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import time\n\n# Compare different algorithms and parameters\n# Optimizations applied:\n# - Dataset sampling: max 10,000 rules for faster comparison\n# - PCA dimensionality reduction: ~140 dims â†’ 50 dims\n# - Mini-Batch K-Means replaces Hierarchical clustering\nprint(f\"Starting algorithm comparison...\")\nprint(f\"Original dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n\nstart_time = time.time()\ncomparison = clustering.compare_algorithms(\n    X, df,\n    max_samples=10000,  # Limit to 10k samples for speed\n    apply_pca_reduction=True,  # Apply PCA\n    pca_components=50  # Reduce to 50 dimensions\n)\nelapsed_time = time.time() - start_time\n\nprint(f\"\\nAlgorithm comparison completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\nprint(\"\\nAlgorithm Comparison:\")\ncomparison",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot silhouette scores\n",
    "valid_silhouette = comparison.dropna(subset=['silhouette_score'])\n",
    "if not valid_silhouette.empty:\n",
    "    axes[0].barh(valid_silhouette['algorithm'], valid_silhouette['silhouette_score'])\n",
    "    axes[0].set_xlabel('Silhouette Score')\n",
    "    axes[0].set_title('Silhouette Score Comparison (Higher is Better)')\n",
    "\n",
    "# Plot number of clusters\n",
    "axes[1].barh(comparison['algorithm'], comparison['n_clusters'])\n",
    "axes[1].set_xlabel('Number of Clusters')\n",
    "axes[1].set_title('Number of Clusters Found')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Best Clustering Result\n",
    "\n",
    "Choose the best performing algorithm and analyze its clusters in detail."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select the best clusterer (adjust based on comparison above)\n",
    "best_clusterer = kmeans_clusterer  # or dbscan_clusterer or hierarchical_clusterer\n",
    "best_labels = best_clusterer.labels_\n",
    "\n",
    "# Add cluster labels to DataFrame\n",
    "df['cluster'] = best_labels\n",
    "\n",
    "print(f\"Using clustering algorithm: {best_clusterer.algorithm}\")\n",
    "print(f\"Number of clusters: {len(set(best_labels))}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Analyze cluster sizes\n",
    "cluster_sizes = df['cluster'].value_counts().sort_index()\n",
    "print(\"Cluster sizes:\")\n",
    "print(cluster_sizes)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "cluster_sizes.plot(kind='bar')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Rules')\n",
    "plt.title('Rules per Cluster')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sample rules from each cluster\n",
    "msg_col = 'msg' if 'msg' in df.columns else 'message'\n",
    "\n",
    "print(\"Sample rules from each cluster:\\n\")\n",
    "for cluster_id in sorted(df['cluster'].unique()):\n",
    "    if cluster_id == -1:\n",
    "        print(f\"Cluster {cluster_id} (NOISE):\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id}:\")\n",
    "    \n",
    "    cluster_rules = df[df['cluster'] == cluster_id]\n",
    "    sample_messages = cluster_rules[msg_col].head(3).tolist()\n",
    "    \n",
    "    for i, msg in enumerate(sample_messages, 1):\n",
    "        print(f\"  {i}. {msg}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Save Results"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save DataFrame with cluster labels\n",
    "df.to_pickle('../data/clustered_rules.pkl')\n",
    "df.to_csv('../data/clustered_rules.csv', index=False)\n",
    "\n",
    "# Save cluster labels separately\n",
    "np.save('../data/cluster_labels.npy', best_labels)\n",
    "\n",
    "print(\"Clustering results saved\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Generate Cluster Descriptions\n\nGenerate comprehensive descriptions for each cluster with automatic labeling, key terms, and representative rules."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Recreate feature extractor to get TF-IDF vocabulary\nprint(\"Recreating feature extractor for TF-IDF term extraction...\")\nextractor = features.RuleFeatureExtractor()\nX_temp = extractor.create_feature_matrix(df, include_tfidf=True, tfidf_max_features=100)\nprint(\"Feature extractor ready\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Generate cluster descriptions\nprint(\"Generating cluster descriptions...\")\ndescriptor = ClusterDescriptor(feature_extractor=extractor)\ncluster_centers = best_clusterer.get_cluster_centers()\ndescriptions = descriptor.describe_all_clusters(X, best_labels, df, cluster_centers)\nprint(f\"Generated descriptions for {len(descriptions)} clusters\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Display cluster descriptions (showing first 5 for brevity)\nprint(\"=\" * 80)\nprint(\"CLUSTER DESCRIPTIONS\")\nprint(\"=\" * 80)\nprint()\nfor cluster_id in sorted(list(descriptions.keys())[:5]):\n    print(format_cluster_description(descriptions[cluster_id]))\n    print(\"\\n\")\nprint(f\"\\n... (showing 5 of {len(descriptions)} clusters)\")\nprint(\"\\nFor full descriptions, see outputs/cluster_descriptions.md or run 05_cluster_analysis.ipynb\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Export descriptions to files\nprint(\"Exporting cluster descriptions...\")\ndescriptor.export_to_markdown(descriptions, '../outputs/cluster_descriptions.md')\ndescriptor.export_to_csv(descriptions, '../outputs/cluster_summary.csv')\nprint(\"\\nCluster descriptions exported!\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
